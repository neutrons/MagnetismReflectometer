# pylint: disable=bare-except, dangerous-default-value, wrong-import-position, wrong-import-order, too-many-arguments, too-many-instance-attributes
# pylint: disable=bare-except, dangerous-default-value, wrong-import-position, wrong-import-order, too-many-arguments, too-many-instance-attributes
"""
Reduction for MR
"""

# standard imports
import os
import sys
import time
from typing import List, Optional

# third-party imports
# from .settings import MANTID_PATH
# sys.path.insert(0, MANTID_PATH)
import mantid
from mantid.simpleapi import (
    FilterByLogValue,
    GroupWorkspaces,
    LoadEventNexus,
    MagnetismReflectometryReduction,
    MRFilterCrossSections,
    SaveNexus,
    logger,
    mtd,
)

# mr_reduction imports
from mr_reduction.data_info import DataInfo, DataType
from mr_reduction.mr_direct_beam_finder import DirectBeamFinder
from mr_reduction.reflectivity_merge import combined_catalog_info, combined_curves, plot_combined
from mr_reduction.reflectivity_output import write_reflectivity
from mr_reduction.runpeak import RunPeakNumber
from mr_reduction.script_output import write_partial_script
from mr_reduction.settings import ANA_STATE, ANA_VETO, GLOBAL_AR_DIR, POL_STATE, POL_VETO, ar_out_dir
from mr_reduction.simple_utils import SampleLogs
from mr_reduction.types import MantidWorkspace
from mr_reduction.web_report import Report, process_collection

DIRECT_BEAM_EVTS_MIN = 1000


class ReductionProcess:
    """
    MR automated reduction
    """

    tolerance = 0.02
    # Minimum number of events needed to go ahead with the reduction
    min_number_events = 200
    pol_state = POL_STATE
    pol_veto = POL_VETO
    ana_state = ANA_STATE
    ana_veto = ANA_VETO

    def __init__(
        self,
        data_run,
        data_ws=None,
        output_dir=None,
        use_sangle=True,
        const_q_binning=False,
        q_step=-0.02,
        const_q_cutoff=0.02,
        update_peak_range=False,
        peak_number: Optional[int] = None,
        use_roi=True,
        force_peak_roi=False,
        peak_roi=[0, 0],
        use_roi_bck=False,
        force_bck_roi=False,
        bck_roi=[0, 0],
        low_res_roi: List[int] = None,
        force_low_res: bool = False,
        use_tight_bck=False,
        bck_offset=3,
        plot_2d=False,
        publish=True,
        debug=False,
        live=False,
    ):
        r"""
        The automated reduction is initializable such that most of what we need can be
        changed at initialization time. That way the post-processing framework only
        needs to create the object and execute the reduction.

        Parameters
        ----------
        data_run: str
            Run number or file path
        data_ws: mantid.dataobjects.EventWorkspace
            Mantid events workspace containing the events for the run we're to reduce
        output_dir: str
            Directory where the autoreduced files are stored. Usually /SNS/REF_M/IPTS-****/shared/
        use_sangle: bool
            Use metadata entry SANGLE as the scattering angle
        const_q_binning: bool
            Convert to Q before aggregating pixel intensities
        q_step: float
            Step size in Q. Enter a negative value to get a log scale
        const_q_cutoff: float
            (Unused)
        update_peak_range: bool
            Fine tune the peak ROI by fitting a peak within the ROI, and updating the ROI afterwards
        peak_number
            Peak number when the run contains more than one peak. Numbers start at 1 (not 0)
        use_roi: bool

        use_roi_bck
        use_tight_bck
        bck_offset
        force_peak_roi
        peak_roi
        force_bck_roi
        bck_roi
        low_res_roi
            The Y-Pixel-Axis, vertical, or low-resolution range .Pass a two-item list [y_min, y_max]
        force_low_res
            If `True`, use `low_res_roi` to override the low-resolution range the can be computed from the sample logs
        plot_2d: bool
            Create 2D plot of the detector pixel intensities
        publish: bool
            If `True`, try to upload the HTML report generated by `reduce()` into the livedata server
        debug
        live
        """

        try:
            int(data_run)
            self.run_number: Optional[int] = int(data_run)
            self.file_path = "REF_M_%s" % data_run
        except:  # noqa E722
            self.run_number = None
            self.file_path = data_run
        self.data_ws = data_ws
        self.peak_number = None if peak_number is None else int(peak_number)
        self.ipts = None
        self.output_dir = output_dir
        self.const_q_binning = const_q_binning
        # Q-value below which const-q binning will not be used [NOT CURRENTLY IMPLEMENTED]
        self.const_q_cutoff = const_q_cutoff

        # Options
        self.use_roi = use_roi
        self.use_sangle = use_sangle
        self.update_peak_range = update_peak_range
        self.use_roi_bck = use_roi_bck
        self.use_tight_bck = use_tight_bck
        self.bck_offset = bck_offset
        self.q_step = q_step

        # Options to override the ROI
        self.force_peak_roi = force_peak_roi
        self.forced_peak_roi = peak_roi
        self.force_bck_roi = force_bck_roi
        self.forced_bck_roi = bck_roi
        self.force_low_res_roi = force_low_res
        self.forced_low_res_roi = low_res_roi

        self.use_slow_flipper_log = False
        self.publish = publish
        self.live = live
        self.json_info = None

        # Script for re-running the reduction
        self.script = ""
        self.logfile = None
        if debug:
            self.logfile = open(os.path.join(GLOBAL_AR_DIR, "MR_live.log"), "a")
        self.plot_2d = plot_2d

    def log(self, msg):
        """Debug logging"""
        if self.logfile:
            self.logfile.write(msg + "\n")
        logger.notice(msg)

    def _extract_data_info(self, xs_list: List[MantidWorkspace]):
        """
        Extract data info for the cross-section with the most events
        :param list xs_list: workspace group
        """
        # Find the cross-section with the most events
        n_max_events = 0
        i_main = 0
        for i in range(len(xs_list)):
            n_events = xs_list[i].getNumberEvents()
            if n_events > n_max_events:
                n_max_events = n_events
                i_main = i
        sample_logs = SampleLogs(xs_list[i_main])
        self.ipts = sample_logs["experiment_identifier"]
        entry = sample_logs["cross_section_id"]
        data_info = DataInfo(
            xs_list[i_main],
            entry,
            use_roi=self.use_roi,
            update_peak_range=self.update_peak_range,
            use_roi_bck=self.use_roi_bck,
            use_tight_bck=self.use_tight_bck,
            bck_offset=self.bck_offset,
            force_peak_roi=self.force_peak_roi,
            peak_roi=self.forced_peak_roi,
            force_bck_roi=self.force_bck_roi,
            bck_roi=self.forced_bck_roi,
            low_res_roi=self.forced_low_res_roi,
            force_low_res=self.force_low_res_roi,
        )
        # Find direct beam information
        norm_run = None
        direct_info = data_info
        apply_norm = False
        if not data_info.is_direct_beam:
            apply_norm, norm_run, direct_info = self.find_direct_beam(xs_list[i_main])
            if direct_info is None:
                direct_info = data_info

        # Set output directory
        if self.output_dir is None:
            self.output_dir = ar_out_dir(self.ipts)

        # Important note: data_info is created from the cross-section with the most
        # data, so data_info.cross_section indicates which one that was.
        return data_info, direct_info, apply_norm, norm_run

    def slow_filter_cross_sections(self, ws):
        """
        Filter events according to an aggregated state log.
        :param str file_path: file to read

        BL4A:SF:ICP:getDI

        015 (0000 1111): SF1=OFF, SF2=OFF, SF1Veto=OFF, SF2Veto=OFF
        047 (0010 1111): SF1=ON, SF2=OFF, SF1Veto=OFF, SF2Veto=OFF
        031 (0001 1111): SF1=OFF, SF2=ON, SF1Veto=OFF, SF2Veto=OFF
        063 (0011 1111): SF1=ON, SF2=ON, SF1Veto=OFF, SF2Veto=OFF
        """
        state_log = "BL4A:SF:ICP:getDI"
        states = {"Off_Off": 15, "On_Off": 47, "Off_On": 31, "On_On": 63}
        cross_sections = []

        for pol_state in states:
            try:
                _ws = FilterByLogValue(
                    InputWorkspace=ws,
                    LogName=state_log,
                    TimeTolerance=0.1,
                    MinimumValue=states[pol_state],
                    MaximumValue=states[pol_state],
                    LogBoundary="Left",
                    OutputWorkspace="%s_%s" % (ws.getRunNumber(), pol_state),
                )
                _ws.getRun()["cross_section_id"] = pol_state  # add new entry or assign to entry
                if _ws.getNumberEvents() > 0:
                    cross_sections.append(_ws)
            except:  # noqa E722
                mantid.logger.error("Could not filter %s: %s" % (pol_state, sys.exc_info()[1]))

        return cross_sections

    def reduce(self):
        """
        Perform the reduction
        """
        self.log("\n\n---------- %s" % time.ctime())
        # Load cross-sections
        _filename = None if self.data_ws is not None else self.file_path
        # if self.data_ws is not None and self.use_slow_flipper_log:
        if self.data_ws is None:
            self.data_ws = LoadEventNexus(Filename=self.file_path, OutputWorkspace="raw_events")
        self.run_number = int(self.data_ws.getRunNumber())

        if self.use_slow_flipper_log:
            _xs_list = self.slow_filter_cross_sections(self.data_ws)
        else:
            _xs_list = MRFilterCrossSections(
                Filename=_filename,
                InputWorkspace=self.data_ws,
                PolState=self.pol_state,
                AnaState=self.ana_state,
                PolVeto=self.pol_veto,
                AnaVeto=self.ana_veto,
                CrossSectionWorkspaces="%s" % self.data_ws.getRunNumber(),
            )
            # If we have no cross section info, treat the data as unpolarized and use Off_Off as the label.
            for ws in _xs_list:
                if "cross_section_id" not in ws.getRun():
                    ws.getRun()["cross_section_id"] = "Off_Off"  # assign to entry
        xs_list = [
            ws
            for ws in _xs_list
            if not SampleLogs(ws)["cross_section_id"] == "unfiltered" and ws.getNumberEvents() > 0
        ]

        # Reduce each cross-section
        report_list = self.reduce_workspace_group(xs_list)

        # Generate stitched plot
        ref_plot = None
        try:
            run_peak_number = str(RunPeakNumber(self.run_number, self.peak_number))
            matched_runs, scaling_factors, outputs = combined_curves(
                run=run_peak_number, ipts=self.ipts, output_dir=self.output_dir
            )
            if not self.live:
                self.json_info = combined_catalog_info(
                    matched_runs,
                    self.ipts,
                    outputs,
                    output_dir=self.output_dir,
                    run_peak_number=str(RunPeakNumber(self.run_number, self.peak_number)),
                )
            self.log("Matched runs: %s" % str(matched_runs))
            # plotly figures for the reflectivity profile of each cross section, and embed them in an <div> container
            ref_plot = plot_combined(
                matched_runs, scaling_factors, self.ipts, extra_search_dir=self.output_dir, publish=False
            )
            self.log("Generated reflectivity: %s" % len(str(ref_plot)))
        except:  # noqa E722
            self.log("Could not generate combined curve")
            self.log(str(sys.exc_info()[1]))
            logger.error(str(sys.exc_info()[1]))

        # Generate report and script
        logger.notice("Processing collection of %s reports" % len(report_list))
        try:
            html_report, _ = process_collection(
                summary_content=ref_plot,
                report_list=report_list,
                publish=self.publish,
                run_number=str(self.run_number),
            )
        except:  # noqa E722
            html_report = ""
            self.log("Could not process reports %s" % sys.exc_info()[1])

        if self.logfile:
            self.logfile.close()
        return html_report

    def reduce_workspace_group(self, xs_list: List[MantidWorkspace]):
        # Extract data info (find peaks, etc...)
        # This can be moved within the for-loop below re-extraction with each cross-section.
        # Generally, the peak ranges should be consistent between cross-section.
        data_info, direct_info, apply_norm, norm_run = self._extract_data_info(xs_list)
        self.log(f"Norm run: {norm_run}")

        # Determine the name of the direct beam workspace as needed
        ws_norm = direct_info.workspace_name if apply_norm and norm_run is not None else ""

        # Find reflectivity peak of scattering run
        ws = xs_list[0]
        sample_logs = SampleLogs(ws)
        entry = sample_logs["cross_section_id"]
        self.ipts = sample_logs["experiment_identifier"]

        # combine run and peak number when the run contains more than one peak
        runpeak = RunPeakNumber(self.run_number, self.peak_number)
        logger.notice(
            "R%s [%s] DATA TYPE: %s [ref=%s] [%s events]"
            % (runpeak, entry, data_info.data_type.name, data_info.cross_section, ws.getNumberEvents())
        )
        self.log(
            "R%s [%s] DATA TYPE: %s [ref=%s] [%s events]"
            % (runpeak, entry, data_info.data_type.name, data_info.cross_section, ws.getNumberEvents())
        )
        if (data_info.data_type != DataType.REFLECTED_BEAM) or (ws.getNumberEvents() < self.min_number_events):
            self.log(
                "  - skipping: data type=%s; events: %s [cutoff: %s]"
                % (data_info.data_type.name, ws.getNumberEvents(), self.min_number_events)
            )
            return [Report(ws, data_info, data_info, None, logfile=self.logfile, plot_2d=self.plot_2d)]

        wsg = GroupWorkspaces(InputWorkspaces=xs_list)
        MagnetismReflectometryReduction(
            InputWorkspace=wsg,
            NormalizationWorkspace=ws_norm,
            SignalPeakPixelRange=data_info.peak_range,
            SubtractSignalBackground=True,
            SignalBackgroundPixelRange=data_info.background,
            ApplyNormalization=apply_norm,
            NormPeakPixelRange=direct_info.peak_range,
            SubtractNormBackground=True,
            NormBackgroundPixelRange=direct_info.background,
            CutLowResDataAxis=True,
            LowResDataAxisPixelRange=data_info.low_res_range,
            CutLowResNormAxis=True,
            LowResNormAxisPixelRange=direct_info.low_res_range,
            CutTimeAxis=True,
            QMin=0.001,
            QStep=self.q_step,
            UseWLTimeAxis=False,
            TimeAxisStep=40,
            UseSANGLE=self.use_sangle,
            TimeAxisRange=data_info.tof_range,
            SpecularPixel=data_info.peak_position,
            ConstantQBinning=self.const_q_binning,
            ConstQTrim=0.1,
            OutputWorkspace=f"r_{runpeak}",
        )

        # Save peak number in the logs of the reduced workspaces
        if runpeak.peak_number:
            runpeak.log_peak_number(f"r_{runpeak}")

        # Generate partial python script
        self.log("Workspace r_%s: %s" % (runpeak, type(mtd["r_%s" % runpeak])))
        write_partial_script(mtd["r_%s" % runpeak], output_dir=self.output_dir)

        report_list = []
        for ws in xs_list:
            try:
                if str(ws).endswith("unfiltered"):
                    continue
                self.log(f"\n--- Run {runpeak} {str(ws)} ---\n")
                entry = SampleLogs(ws)["cross_section_id"]
                reflectivity = mtd["%s__reflectivity" % str(ws)]
                report = Report(ws, data_info, direct_info, reflectivity, logfile=self.logfile, plot_2d=self.plot_2d)
                report_list.append(report)

                # Write output file in QuickNXS format
                self.log("  - ready to write: %s" % self.output_dir)
                write_reflectivity(
                    [reflectivity],
                    os.path.join(self.output_dir, "REF_M_%s_%s_autoreduce.dat" % (runpeak, entry)),
                    data_info.cross_section_label,
                )

                # Write output file in NeXus format
                SaveNexus(
                    InputWorkspace=reflectivity,
                    Filename=os.path.join(self.output_dir, "REF_M_%s_%s_autoreduce.nxs.h5" % (runpeak, entry)),
                )

                self.log("  - done writing")
                # Write partial output script
            except:  # noqa E722
                self.log("  - reduction failed")
                # No data for this cross-section, skip to the next
                logger.error("Cross section: %s" % str(sys.exc_info()[1]))
                report = Report(ws, data_info, direct_info, None, plot_2d=self.plot_2d)
                report_list.append(report)

        return report_list

    def find_direct_beam(self, scatt_ws):
        """
        Find the appropriate direct beam run
        :param workspace scatt_ws: scattering workspace we are trying to match
        """
        run_number = scatt_ws.getRunNumber()
        entry = SampleLogs(scatt_ws)["cross_section_id"]
        db_finder = DirectBeamFinder(scatt_ws, skip_slits=False, tolerance=self.tolerance, experiment=self.ipts)
        norm_run = db_finder.search()
        if norm_run is None:
            logger.warning(
                "Run %s [%s]: Could not find direct beam with matching slit, trying with wl only" % (run_number, entry)
            )
            norm_run = db_finder.search(skip_slits=True)
        apply_norm = False
        direct_info = None
        if norm_run is None:
            logger.warning("Run %s [%s]: Could not find direct beam run: skipping" % (run_number, entry))
        else:
            logger.notice("Run %s [%s]: Direct beam run: %s" % (run_number, entry, norm_run))

            # Find peak in direct beam run
            for norm_entry in ["entry", "entry-Off_Off", "entry-On_Off", "entry-Off_On", "entry-On_On"]:
                try:
                    ws_direct = LoadEventNexus(
                        Filename="REF_M_%s" % norm_run, NXentryName=norm_entry, OutputWorkspace="MR_%s" % norm_run
                    )
                    if ws_direct.getNumberEvents() > DIRECT_BEAM_EVTS_MIN:
                        direct_info = DataInfo(
                            ws_direct,
                            norm_entry,
                            use_roi=self.use_roi,
                            update_peak_range=True,  # self.update_peak_range,
                            use_roi_bck=self.use_roi_bck,
                            use_tight_bck=self.use_tight_bck,
                            bck_offset=self.bck_offset,
                        )
                        apply_norm = True
                        break
                except:  # noqa E722
                    # No data in this cross-section
                    logger.error("Direct beam %s: %s" % (norm_entry, sys.exc_info()[1]))
        return apply_norm, norm_run, direct_info
