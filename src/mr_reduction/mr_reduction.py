"""
Reduction for MR
"""

import os
import time
from io import IOBase
from typing import List, Optional

from mantid.simpleapi import (
    GroupWorkspaces,
    LoadEventNexus,
    MagnetismReflectometryReduction,
    SaveNexus,
    logger,
    mtd,
)

from mr_reduction import io_orso
from mr_reduction.data_info import DataInfo, DataType
from mr_reduction.filter_events import split_events
from mr_reduction.mr_direct_beam_finder import DirectBeamFinder
from mr_reduction.reflectivity_merge import combined_catalog_info, combined_curves, plot_combined
from mr_reduction.reflectivity_output import write_reflectivity
from mr_reduction.runpeak import RunPeakNumber
from mr_reduction.script_output import write_partial_script
from mr_reduction.settings import GLOBAL_AR_DIR, PolarizationLogs
from mr_reduction.simple_utils import SampleLogs
from mr_reduction.types import MantidWorkspace
from mr_reduction.web_report import Report, process_collection

DIRECT_BEAM_EVTS_MIN = 1000
REFLECTED_BEAM_EVTS_MIN = 200


class ReductionProcess:
    """
    MR automated reduction
    """

    tolerance = 0.02
    # Minimum number of events needed to go ahead with the reduction
    min_number_events = REFLECTED_BEAM_EVTS_MIN
    polarization_logs = PolarizationLogs()

    def __init__(
        self,
        data_run,
        data_ws=None,
        output_dir=None,
        use_sangle=True,
        const_q_binning=False,
        q_step=-0.02,
        const_q_cutoff=0.02,
        update_peak_range=False,
        peak_number: Optional[int] = None,
        use_roi=True,
        force_peak_roi=False,
        peak_roi=[0, 0],
        use_roi_bck=False,
        force_bck_roi=False,
        bck_roi=[0, 0],
        low_res_roi: List[int] = None,
        force_low_res: bool = False,
        use_tight_bck=False,
        bck_offset=3,
        publish=True,
        debug: bool = False,
        logfile: str = None,
        live: bool = False,
    ):
        r"""
        The automated reduction is initializable such that most of what we need can be
        changed at initialization time. That way the post-processing framework only
        needs to create the object and execute the reduction.

        Parameters
        ----------
        data_run: str
            Run number or file path
        data_ws: mantid.dataobjects.EventWorkspace
            Mantid events workspace containing the events for the run we're to reduce
        output_dir: str
            Directory where the autoreduced files will be stored.
            If `None`, it will be set to /SNS/REF_M/IPTS-****/shared/autoreduce
        use_sangle: bool
            Use metadata entry SANGLE as the scattering angle
        const_q_binning: bool
            Convert to Q before aggregating pixel intensities
        q_step: float
            Step size in Q. Enter a negative value to get a log scale
        const_q_cutoff: float
            (Unused)
        update_peak_range: bool
            Fine tune the peak ROI by fitting a peak within the ROI, and updating the ROI afterwards
        peak_number
            Peak number when the run contains more than one peak. Numbers start at 1 (not 0)
        use_roi: bool

        use_roi_bck
        use_tight_bck
        bck_offset
        force_peak_roi
        peak_roi
        force_bck_roi
        bck_roi
        low_res_roi
            The Y-Pixel-Axis, vertical, or low-resolution range .Pass a two-item list [y_min, y_max]
        force_low_res
            If `True`, use `low_res_roi` to override the low-resolution range the can be computed from the sample logs
        publish: bool
            If `True`, try to upload the HTML report generated by `reduce()` into the livedata server
        debug: bool
            If `True`, write log messages to file /SNS/REF_M/shared/autoreduce/reduce_REF_M.log
        logfile: str
            file path to write log messages to. If passed, `debug` is ignored.
        live: bool
            If `True`, the reduction is treated as a live reduction, that is, reduction before the run is finished.
        """

        try:
            self.run_number: Optional[int] = int(data_run)
            self.file_path = f"REF_M_{data_run}"
        except (TypeError, ValueError):
            self.run_number = None
            self.file_path = data_run
        self.data_ws = data_ws
        self.peak_number = None if peak_number is None else int(peak_number)
        self.ipts = None
        if output_dir is not None:
            os.makedirs(output_dir, exist_ok=True)  # create the directory
        self.output_dir = output_dir
        self.const_q_binning = const_q_binning
        # Q-value below which const-q binning will not be used [NOT CURRENTLY IMPLEMENTED]
        self.const_q_cutoff = const_q_cutoff

        # Options
        self.use_roi = use_roi
        self.use_sangle = use_sangle
        self.update_peak_range = update_peak_range
        self.use_roi_bck = use_roi_bck
        self.use_tight_bck = use_tight_bck
        self.bck_offset = bck_offset
        self.q_step = q_step

        # Options to override the ROI
        self.force_peak_roi = force_peak_roi
        self.forced_peak_roi = peak_roi
        self.force_bck_roi = force_bck_roi
        self.forced_bck_roi = bck_roi
        self.force_low_res_roi = force_low_res
        self.forced_low_res_roi = low_res_roi

        self.use_slow_flipper_log = False
        self.publish = publish
        self.live = live
        self.json_info = None

        # Script for re-running the reduction
        self.script = ""

        # "REF_M_livereduce.log" or "REF_M_autoreduce.log"
        self.logfile: Optional[IOBase] = None  # a file handle to a log file
        if logfile:  # usually path to "REF_M_livereduce.log"
            self.logfile = open(logfile, "a")
        elif debug is True:
            self.logfile = open(os.path.join(GLOBAL_AR_DIR, "REF_M_autoreduce.log"), "a")

    def log(self, msg, level="notice"):
        """Debug logging"""
        if self.logfile:
            self.logfile.write(msg + "\n")
        getattr(logger, level)(msg)

    def set_ipts(self, ipts):
        """
        Set the IPTS (Integrated Proposal Tracking System) identifier and update the autoreduction directory.

        This method sets the IPTS identifier for the current reduction process.
        Also, `output_dir` is set to '/SNS/REF_M/{ipts}/shared/autoreduce/' if found empty.

        Parameters
        ----------
        ipts : str
            The experiment identifier (e.g., "IPTS-12345").
        """
        if self.ipts is not None:
            return  # we already have an IPTS
        assert "IPTS-" in ipts, f"Invalid IPTS identifier: {ipts}"
        self.ipts = ipts
        if self.output_dir is None:
            self.output_dir = f"/SNS/REF_M/{ipts}/shared/autoreduce/"

    def _extract_data_info(self, xs_list: List[MantidWorkspace]):
        """
        Extract data info for the cross-section with the most events
        :param list xs_list: workspace group
        """
        # Find the cross-section with the most events
        n_max_events = 0
        i_main = 0
        for i in range(len(xs_list)):
            n_events = xs_list[i].getNumberEvents()
            if n_events > n_max_events:
                n_max_events = n_events
                i_main = i
        sample_logs = SampleLogs(xs_list[i_main])
        self.set_ipts(sample_logs["experiment_identifier"])
        entry = sample_logs["cross_section_id"]
        data_info = DataInfo(
            xs_list[i_main],
            entry,
            peak_number=self.peak_number,
            use_roi=self.use_roi,
            update_peak_range=self.update_peak_range,
            use_roi_bck=self.use_roi_bck,
            use_tight_bck=self.use_tight_bck,
            bck_offset=self.bck_offset,
            force_peak_roi=self.force_peak_roi,
            peak_roi=self.forced_peak_roi,
            force_bck_roi=self.force_bck_roi,
            bck_roi=self.forced_bck_roi,
            low_res_roi=self.forced_low_res_roi,
            force_low_res_roi=self.force_low_res_roi,
        )
        # Find direct beam information
        norm_run = None
        direct_info = data_info
        apply_norm = False
        if data_info.is_direct_beam is False:
            apply_norm, norm_run, direct_info = self.find_direct_beam(xs_list[i_main])
            if direct_info is None:
                direct_info = data_info

        # Important note: data_info is created from the cross-section with the most
        # data, so data_info.cross_section indicates which one that was.
        return data_info, direct_info, apply_norm, norm_run

    def reduce(self):
        """
        Perform the reduction
        """
        self.log("\n\n---------- %s" % time.ctime())
        # Load cross-sections
        _filename = None if self.data_ws is not None else self.file_path
        try:
            _xs_list = split_events(
                file_path=_filename,
                input_workspace=self.data_ws,
                min_event_count=self.min_number_events,
                use_slow_flipper_log=self.use_slow_flipper_log,
                polarization_logs=self.polarization_logs,
            )

            if len(_xs_list) == 0:
                raise ValueError("No cross-sections found after filtering")

            # Extract run number from first workspace
            if self.run_number is None:
                self.run_number = int(_xs_list[0].getRunNumber())

            xs_list = [
                ws
                for ws in _xs_list
                if (SampleLogs(ws)["cross_section_id"] != "unfiltered") and (ws.getNumberEvents() > 0)
            ]

            # Reduce each cross-section
            report_list = self.reduce_workspace_group(xs_list)

            # Generate stitched plot
            ref_plot = None
            try:
                run_peak_number = str(RunPeakNumber(self.run_number, self.peak_number))
                matched_run_list, scaling_factor_list, stitched_filepath_list = combined_curves(
                    run=run_peak_number, ipts=self.ipts, ar_dir=self.output_dir
                )
                if self.live is False:
                    self.json_info = combined_catalog_info(
                        matched_run_list,
                        self.ipts,
                        stitched_filepath_list,
                        ar_dir=self.output_dir,
                        run_peak_number=str(RunPeakNumber(self.run_number, self.peak_number)),
                    )
                self.log("Matched runs: %s" % str(matched_run_list))
                # plotly figures for the reflectivity profile of each cross section, and embed in an <div> container
                ref_plot = plot_combined(matched_run_list, scaling_factor_list, self.output_dir, publish=False)
                self.log("Generated reflectivity: %s" % len(str(ref_plot)))
            except Exception as e:  # noqa E722
                self.log("Could not generate combined curve")
                self.log(str(e))
                logger.error(str(e))

            logger.notice("Processing collection of %s reports" % len(report_list))
            html_report, _ = process_collection(summary_content=ref_plot, report_list=report_list)
        finally:
            if self.logfile:
                self.logfile.close()

        return html_report

    def reduce_workspace_group(self, xs_list: List[MantidWorkspace]):
        # Extract data info (find peaks, etc...)
        # This can be moved within the for-loop below re-extraction with each cross-section.
        # Generally, the peak ranges should be consistent between cross-section.
        data_info, direct_info, apply_norm, norm_run = self._extract_data_info(xs_list)
        self.log(f"Norm run: {norm_run}")

        # Determine the name of the direct beam workspace as needed
        ws_norm = direct_info.workspace_name if apply_norm and norm_run is not None else ""

        # Find reflectivity peak of scattering run
        ws = xs_list[0]
        sample_logs = SampleLogs(ws)
        entry = sample_logs["cross_section_id"]
        self.set_ipts(sample_logs["experiment_identifier"])

        # combine run and peak number when the run contains more than one peak
        runpeak = RunPeakNumber(self.run_number, self.peak_number)
        logger.notice(
            "R%s [%s] DATA TYPE: %s [ref=%s] [%s events]"
            % (runpeak, entry, data_info.data_type.name, data_info.cross_section, ws.getNumberEvents())
        )
        self.log(
            "R%s [%s] DATA TYPE: %s [ref=%s] [%s events]"
            % (runpeak, entry, data_info.data_type.name, data_info.cross_section, ws.getNumberEvents())
        )
        if (data_info.data_type != DataType.REFLECTED_BEAM) or (ws.getNumberEvents() < self.min_number_events):
            self.log(
                "  - skipping: data type=%s; events: %s [cutoff: %s]"
                % (data_info.data_type.name, ws.getNumberEvents(), self.min_number_events)
            )
            return [Report(ws, data_info, data_info, None, logfile=self.logfile)]

        wsg = GroupWorkspaces(InputWorkspaces=xs_list)
        MagnetismReflectometryReduction(
            InputWorkspace=wsg,
            NormalizationWorkspace=ws_norm,
            SignalPeakPixelRange=data_info.peak_range,
            SubtractSignalBackground=True,
            SignalBackgroundPixelRange=data_info.background,
            ApplyNormalization=apply_norm,
            NormPeakPixelRange=direct_info.peak_range,
            SubtractNormBackground=True,
            NormBackgroundPixelRange=direct_info.background,
            CutLowResDataAxis=True,
            LowResDataAxisPixelRange=data_info.low_res_range,
            CutLowResNormAxis=True,
            LowResNormAxisPixelRange=direct_info.low_res_range,
            CutTimeAxis=True,
            QMin=0.001,
            QStep=self.q_step,
            UseWLTimeAxis=False,
            TimeAxisStep=40,
            UseSANGLE=self.use_sangle,
            TimeAxisRange=data_info.tof_range,
            SpecularPixel=data_info.peak_position,
            ConstantQBinning=self.const_q_binning,
            ConstQTrim=0.1,
            OutputWorkspace=f"r_{runpeak}",
        )

        # Save peak number in the logs of the reduced workspaces
        if runpeak.peak_number:
            runpeak.log_peak_number(f"r_{runpeak}")

        # Generate partial python script
        self.log("Workspace r_%s: %s" % (runpeak, type(mtd["r_%s" % runpeak])))
        write_partial_script(mtd["r_%s" % runpeak], self.output_dir)

        report_list = []
        reflectivity_workspaces: List[MantidWorkspace] = []
        for ws in xs_list:
            try:
                if str(ws).endswith("unfiltered"):
                    continue
                self.log(f"\n--- Run {runpeak} {str(ws)} ---\n")
                entry = SampleLogs(ws)["cross_section_id"]
                reflectivity = mtd["%s__reflectivity" % str(ws)]
                report = Report(ws, data_info, direct_info, reflectivity, logfile=self.logfile)
                report_list.append(report)
                # Write output file in QuickNXS format
                self.log("  - ready to write: %s" % self.output_dir)
                write_reflectivity(
                    [reflectivity],
                    os.path.join(self.output_dir, "REF_M_%s_%s_autoreduce.dat" % (runpeak, entry)),
                    data_info.cross_section_label,
                )

                # Write output file in NeXus format.
                SaveNexus(
                    InputWorkspace=reflectivity,
                    Filename=os.path.join(self.output_dir, "REF_M_%s_%s_autoreduce.nxs.h5" % (runpeak, entry)),
                )

                reflectivity_workspaces.append(reflectivity)
                self.log("  - done writing")

            except Exception as e:  # noqa E722
                self.log("  - reduction failed")
                # No data for this cross-section, skip to the next
                logger.error(f"Cross section: {str(e)}")
                report = Report(ws, data_info, direct_info, None)
                report_list.append(report)

        # save the reflectivities of all valid cross sections to an ORSO file
        io_orso.save_cross_sections(reflectivity_workspaces, os.path.join(self.output_dir, f"REF_M_{runpeak}.ort"))

        return report_list

    def find_direct_beam(self, scatt_ws: MantidWorkspace):
        """
        Find the appropriate direct beam run
        :param workspace scatt_ws: scattering workspace we are trying to match
        """
        run_number = scatt_ws.getRunNumber()
        entry = SampleLogs(scatt_ws)["cross_section_id"]
        db_finder = DirectBeamFinder(
            scatt_ws, experiment=self.ipts, ar_dir=self.output_dir, skip_slits=False, tolerance=self.tolerance
        )
        norm_run = db_finder.search()
        if norm_run is None:
            logger.warning(
                "Run %s [%s]: Could not find direct beam with matching slit, trying with wl only" % (run_number, entry)
            )
            norm_run = db_finder.search(skip_slits=True)
        apply_norm = False
        direct_info = None
        if norm_run is None:
            logger.warning("Run %s [%s]: Could not find direct beam run: skipping" % (run_number, entry))
        else:
            logger.notice("Run %s [%s]: Direct beam run: %s" % (run_number, entry, norm_run))

            # Find peak in direct beam run
            for norm_entry in ["entry", "entry-Off_Off", "entry-On_Off", "entry-Off_On", "entry-On_On"]:
                try:
                    ws_direct = LoadEventNexus(
                        Filename="REF_M_%s" % norm_run, NXentryName=norm_entry, OutputWorkspace="MR_%s" % norm_run
                    )
                    if ws_direct.getNumberEvents() > DIRECT_BEAM_EVTS_MIN:
                        direct_info = DataInfo(
                            ws_direct,
                            norm_entry,
                            peak_number=1,  # direct-beam runs only have one peak
                            use_roi=self.use_roi,
                            update_peak_range=True,  # self.update_peak_range,
                            use_roi_bck=self.use_roi_bck,
                            use_tight_bck=self.use_tight_bck,
                            bck_offset=self.bck_offset,
                        )
                        apply_norm = True
                        break
                except Exception as e:  # noqa E722
                    # No data in this cross-section
                    logger.error(f"Direct beam {norm_entry}: {str(e)}")
        return apply_norm, norm_run, direct_info
